{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f6a0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c454b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.utils import load_img\n",
    "from keras.utils import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fbceaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(include_top=True)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b666e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(directory):\n",
    "    in_layer = Input(shape=(224, 224, 3))\n",
    "    features = dict()\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        image_id = name.split('.')[0]\n",
    "        features[image_id] = feature\n",
    "        #print('>%s' % name)\n",
    "    return features\n",
    "\n",
    "\n",
    "# extract features from all images\n",
    "directory = r'C:\\Users\\hp\\caption_generator\\flickr8k\\Images'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "# save to file\n",
    "dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1714925d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from os import listdir\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.utils import load_img\n",
    "from keras.utils import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        image_id = image_id.split('.')[0]\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            desc = desc.split()\n",
    "            desc = [word.lower() for word in desc]\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            # remove tokens with numbers in them\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            desc_list[i] =  ' '.join(desc)\n",
    "\n",
    "#  a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "filename = r'C:\\Users\\hp\\caption_generator\\Flickr8k_text\\Flickr8k.token.txt'\n",
    "doc = load_doc(filename)\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "clean_descriptions(descriptions)\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "#save_descriptions(descriptions, 'descriptions1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5b0374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "\n",
    "\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in dataset:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = r'C:\\Users\\hp\\caption_generator\\Flickr8k_text\\Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions(r'C:\\Users\\hp\\caption_generator\\Flickr8k_text\\descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features(r\"C:\\Users\\hp\\caption_generator\\Flickr8k_text\\features .pkl\", train)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d562422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7579\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# prepare tokenizer\n",
    "token = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(token.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "#dump(token, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e97c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6015b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Length: 34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    " \n",
    "\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "# prepare sequences -not , choose progressive loading \n",
    "#X1train, X2train, ytrain = create_sequences(token, max_length, train_descriptions, train_features, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb731bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 4096)\n",
      "(47, 34)\n",
      "(47, 7579)\n"
     ]
    }
   ],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [in_img, in_seq], out_word\n",
    "\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "\t\t# encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    " \n",
    "# test the data generator\n",
    "generator = data_generator(train_descriptions, train_features, token, max_length, vocab_size)\n",
    "inputs, outputs = next(generator)\n",
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faf96d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 34)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 34, 256)      1940224     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096)         0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 34, 256)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          1048832     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          525312      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 7579)         1947803     ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_8476\\594507756.py:33: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 8109s 1s/step - loss: 4.6629\n",
      "6000/6000 [==============================] - 7687s 1s/step - loss: 3.8920\n",
      "6000/6000 [==============================] - 7420s 1s/step - loss: 3.6396\n",
      "6000/6000 [==============================] - 6456s 1s/step - loss: 3.4906\n",
      "6000/6000 [==============================] - 6434s 1s/step - loss: 3.3982\n",
      "6000/6000 [==============================] - 6462s 1s/step - loss: 3.3356\n",
      "6000/6000 [==============================] - 6562s 1s/step - loss: 3.2820\n",
      "6000/6000 [==============================] - 9666s 2s/step - loss: 3.2412\n",
      "6000/6000 [==============================] - 7400s 1s/step - loss: 3.2199\n",
      "6000/6000 [==============================] - 7619s 1s/step - loss: 3.1828\n",
      "6000/6000 [==============================] - 8719s 1s/step - loss: 3.1560\n",
      "6000/6000 [==============================] - 8611s 1s/step - loss: 3.1369\n",
      "6000/6000 [==============================] - 8148s 1s/step - loss: 3.1239\n",
      "6000/6000 [==============================] - 7109s 1s/step - loss: 3.1151\n",
      "6000/6000 [==============================] - 6736s 1s/step - loss: 3.1064\n",
      "6000/6000 [==============================] - 6787s 1s/step - loss: 3.0952\n",
      "6000/6000 [==============================] - 6840s 1s/step - loss: 3.0907\n",
      "6000/6000 [==============================] - 6873s 1s/step - loss: 3.0815\n",
      "6000/6000 [==============================] - 6933s 1s/step - loss: 3.0843\n",
      "6000/6000 [==============================] - 6990s 1s/step - loss: 3.0847\n"
     ]
    }
   ],
   "source": [
    "#Saved -no execution\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    " # create the data generator\n",
    " generator = data_generator(train_descriptions, train_features, token, max_length, vocab_size)\n",
    " # fit for one epoch\n",
    " model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    " # save model\n",
    " model.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a390589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model \n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        \n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc7c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n",
      "BLEU-1: 0.518544\n",
      "BLEU-2: 0.270265\n",
      "BLEU-3: 0.181584\n",
      "BLEU-4: 0.081229\n"
     ]
    }
   ],
   "source": [
    "# evaluate the skill of the model\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "        \n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    " \n",
    "\n",
    "# prepare test set\n",
    "filename = r'C:\\Users\\hp\\caption_generator\\Flickr8k_text\\Flickr_8k.testImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "\n",
    "test_descriptions = load_clean_descriptions(r'C:\\Users\\hp\\caption_generator\\Flickr8k_text\\descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "\n",
    "test_features = load_photo_features(r\"C:\\Users\\hp\\caption_generator\\Flickr8k_text\\features .pkl\", test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    " \n",
    "# load the model\n",
    "filename = r\"C:\\Users\\hp\\caption_generator\\model_17.h5\"\n",
    "model_caption = load_model(filename)\n",
    "# evaluate model\n",
    "evaluate_model(model_caption, test_descriptions, test_features, token, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c517f8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq man in black shirt is standing on bench endseq\n"
     ]
    }
   ],
   "source": [
    "#create captions\n",
    "def extract_features_this(filename):\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def generate_desc_this(model, tokenizer, photo, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = load(open(r\"C:\\Users\\hp\\caption_generator\\tokenizer.pkl\", 'rb'))\n",
    "max_length = 34\n",
    "\n",
    "\n",
    "#photo = extract_features_this(r\"C:\\Users\\hp\\AIML\\dog.png\")\n",
    "#photo= extract_features_this(r\"C:\\Users\\hp\\caption_generator\\flickr8k\\Images\\44856031_0d82c2c7d1.jpg\")\n",
    " \n",
    "photo = extract_features_this(r\"C:\\Users\\hp\\AIML\\hulgjayakfo.jpg\")\n",
    "description = generate_desc_this(model_caption, tokenizer, photo, max_length)\n",
    "print(description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ede16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
